{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "dataset = \"ohsumed\"\n",
    "save_path = 'temp_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "window_size = 3\n",
    "embedding_dim = 300\n",
    "max_text_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "def normalize_adj(adj):\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    # 忽略除零\n",
    "    with np.errstate(divide='ignore'):\n",
    "        d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    adj_normalized = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "    return adj_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, pad_len):\n",
    "    if len(seq) > pad_len:\n",
    "        return seq[:pad_len]\n",
    "    return seq + [0] * (pad_len - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same(target, words):\n",
    "    b = []\n",
    "    for index, nums in enumerate(words):\n",
    "        if nums == target:\n",
    "            b.append(index)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_same(words):\n",
    "    temp_edges = []\n",
    "    list_set_words = list(set(words))\n",
    "    for l in list_set_words:\n",
    "        temp = check_same(l, words)\n",
    "        if len(temp) > 1:\n",
    "            temp_edges += list(itertools.permutations(temp, 2))\n",
    "    return temp_edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词典和对应序号\n",
    "word2index = joblib.load(f\"{save_path}/{dataset}.word2index.pkl\")\n",
    "# 词典和对应序号\n",
    "pos2index = joblib.load(f\"{save_path}/{dataset}.pos2index.pkl\")\n",
    "# 数据集\n",
    "with open(f\"{save_path}/{dataset}.texts.remove.txt\", \"r\") as f:\n",
    "    texts = f.read().strip().split(\"\\n\")\n",
    "pos = joblib.load(f\"{save_path}/{dataset}.texts.pos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建图\n",
    "inputs = []\n",
    "graphs = []\n",
    "inputs_pos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7400/7400 [00:23<00:00, 311.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(texts))):\n",
    "    words = [word2index[w] for w in texts[i].split()]\n",
    "    words = words[:max_text_len]\n",
    "    \n",
    "    poses = [pos2index[w] for w in pos[i]]\n",
    "    poses = poses[:max_text_len]\n",
    "              \n",
    "    nodes = words\n",
    "    edges = []\n",
    "    for i in range(len(words)):\n",
    "        center = i\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if i != j and 0 <= j < len(words):\n",
    "                neighbor = j\n",
    "                edges.append((center, neighbor))\n",
    "    cs = connect_same(words)\n",
    "    edges += cs\n",
    "    edge_count = Counter(edges).items()\n",
    "    # 邻接矩阵\n",
    "    row = [x for (x, y), c in edge_count]\n",
    "    col = [y for (x, y), c in edge_count]\n",
    "    weight = [c for (x, y), c in edge_count]\n",
    "    # 归一化\n",
    "    adj = sp.csr_matrix((weight, (row, col)), shape=(len(nodes), len(nodes)))\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    weight_normalized = [adj_normalized[x][y] for (x, y), c in edge_count]\n",
    "    # 保存节点和图\n",
    "    inputs.append(nodes)\n",
    "    graphs.append([row, col, weight_normalized])\n",
    "    inputs_pos.append(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_inputs = [len(e) for e in inputs]\n",
    "len_graphs = [len(x) for x, y, c in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7400/7400 [00:00<00:00, 150061.64it/s]\n",
      "100%|██████████| 7400/7400 [00:01<00:00, 5717.04it/s]\n",
      "100%|██████████| 7400/7400 [00:00<00:00, 240193.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# padding input\n",
    "pad_len_inputs = max(len_inputs)\n",
    "pad_len_graphs = max(len_graphs)\n",
    "inputs_pad = [pad_seq(e, pad_len_inputs) for e in tqdm(inputs)]\n",
    "graphs_pad = [[pad_seq(ee, pad_len_graphs) for ee in e] for e in tqdm(graphs)]\n",
    "poses_pad = [pad_seq(e, pad_len_inputs) for e in tqdm(inputs_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_pad = np.array(inputs_pad)\n",
    "weights_pad = np.array([c for x, y, c in graphs_pad])\n",
    "graphs_pad = np.array([[x, y] for x, y, c in graphs_pad])\n",
    "poses_pad = np.array(poses_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "all_vectors = np.load(f\"source/glove.6B.{embedding_dim}d.npy\")\n",
    "all_words = joblib.load(f\"source/glove.6B.words.pkl\")\n",
    "all_word2index = {w: i for i, w in enumerate(all_words)}\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "word_set = [index2word[i] for i in range(len(index2word))]\n",
    "oov = np.random.normal(-0.01, 0.01, embedding_dim)\n",
    "word2vec = [all_vectors[all_word2index[w]] if w in all_word2index else oov for w in word_set]\n",
    "word2vec.append(np.zeros(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos2vec\n",
    "index2pos = {i: w for w, i in pos2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = [index2pos[i] for i in range(len(index2pos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2vec = [np.random.normal(-0.1, 0.1, embedding_dim) for w in pos_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "joblib.dump(len_inputs, f\"{save_path}/{dataset}.len.inputs.pkl\")\n",
    "joblib.dump(len_graphs, f\"{save_path}/{dataset}.len.graphs.pkl\")\n",
    "np.save(f\"{save_path}/{dataset}.inputs.npy\", inputs_pad)\n",
    "np.save(f\"{save_path}/{dataset}.graphs.npy\", graphs_pad)\n",
    "np.save(f\"{save_path}/{dataset}.weights.npy\", weights_pad)\n",
    "np.save(f\"{save_path}/{dataset}.word2vec.npy\", word2vec)\n",
    "np.save(f\"{save_path}/{dataset}.poses_pad.npy\", poses_pad)\n",
    "np.save(f\"{save_path}/{dataset}.pos2vec.npy\", pos2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 节点 set\n",
    "nodes = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点顺序\n",
    "node2index = {e: i for i, e in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边\n",
    "edges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    # 第一个词在node2index中的位置\n",
    "    center = node2index[words[i]]\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        if i != j and 0 <= j < len(words):\n",
    "            neighbor = node2index[words[j]]\n",
    "            edges.append((center, neighbor))\n",
    "edge_count = Counter(edges).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点 set\n",
    "nodes = words\n",
    "# 边\n",
    "edges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    center = i\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        if i != j and 0 <= j < len(words):\n",
    "            neighbor = j\n",
    "            edges.append((center, neighbor))\n",
    "cs = connect_same(words)\n",
    "edges += cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_count = Counter(edges).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([((0, 1), 2), ((0, 2), 2), ((0, 3), 2), ((1, 0), 2), ((1, 2), 2), ((1, 3), 2), ((1, 4), 2), ((2, 0), 2), ((2, 1), 2), ((2, 3), 2), ((2, 4), 2), ((2, 5), 2), ((3, 0), 2), ((3, 1), 2), ((3, 2), 2), ((3, 4), 2), ((3, 5), 2), ((3, 6), 2), ((4, 1), 2), ((4, 2), 2), ((4, 3), 2), ((4, 5), 2), ((4, 6), 2), ((4, 7), 2), ((5, 2), 2), ((5, 3), 2), ((5, 4), 2), ((5, 6), 2), ((5, 7), 2), ((5, 8), 2), ((6, 3), 2), ((6, 4), 2), ((6, 5), 2), ((6, 7), 2), ((6, 8), 2), ((6, 9), 2), ((7, 4), 2), ((7, 5), 2), ((7, 6), 2), ((7, 8), 2), ((7, 9), 2), ((7, 10), 2), ((8, 5), 2), ((8, 6), 2), ((8, 7), 2), ((8, 9), 2), ((8, 10), 2), ((8, 11), 2), ((9, 6), 2), ((9, 7), 2), ((9, 8), 2), ((9, 10), 2), ((9, 11), 2), ((9, 12), 2), ((10, 7), 2), ((10, 8), 2), ((10, 9), 2), ((10, 11), 2), ((10, 12), 2), ((10, 13), 2), ((11, 8), 2), ((11, 9), 2), ((11, 10), 2), ((11, 12), 2), ((11, 13), 2), ((11, 14), 2), ((12, 9), 2), ((12, 10), 2), ((12, 11), 2), ((12, 13), 2), ((12, 14), 2), ((12, 15), 2), ((13, 10), 2), ((13, 11), 2), ((13, 12), 2), ((13, 14), 2), ((13, 15), 2), ((13, 16), 2), ((14, 11), 2), ((14, 12), 2), ((14, 13), 2), ((14, 15), 2), ((14, 16), 2), ((14, 17), 2), ((15, 12), 2), ((15, 13), 2), ((15, 14), 2), ((15, 16), 2), ((15, 17), 2), ((15, 18), 2), ((16, 13), 2), ((16, 14), 2), ((16, 15), 2), ((16, 17), 2), ((16, 18), 2), ((16, 19), 2), ((17, 14), 2), ((17, 15), 2), ((17, 16), 2), ((17, 18), 2), ((17, 19), 2), ((17, 20), 2), ((18, 15), 2), ((18, 16), 2), ((18, 17), 2), ((18, 19), 2), ((18, 20), 2), ((18, 21), 2), ((19, 16), 2), ((19, 17), 2), ((19, 18), 2), ((19, 20), 2), ((19, 21), 2), ((20, 17), 2), ((20, 18), 2), ((20, 19), 2), ((20, 21), 2), ((21, 18), 2), ((21, 19), 2), ((21, 20), 2), ((2, 9), 1), ((9, 2), 1)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邻接矩阵\n",
    "row = [x for (x, y), c in edge_count]\n",
    "col = [y for (x, y), c in edge_count]\n",
    "weight = [c for (x, y), c in edge_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(len(nodes), len(nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_normalized = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_normalized = [adj_normalized[x][y] for (x, y), c in edge_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 9), (9, 2)]\n",
      "[(2, 9)]\n"
     ]
    }
   ],
   "source": [
    "aa = [2, 9]\n",
    "bb = list(itertools.permutations(aa, 2))\n",
    "print(bb)\n",
    "cc = list(itertools.combinations(aa, 2))\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(PRP) <-- nsubj -- love(VBP)\n",
      "I love\n",
      "love(VBP) <-- ROOT -- love(VBP)\n",
      "love love\n",
      "natural(JJ) <-- amod -- language(NN)\n",
      "natural language\n",
      "language(NN) <-- compound -- technology(NN)\n",
      "language technology\n",
      "processing(NN) <-- compound -- technology(NN)\n",
      "processing technology\n",
      "technology(NN) <-- dobj -- love(VBP)\n",
      "technology love\n",
      "!(.) <-- punct -- love(VBP)\n",
      "! love\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( \"I love natural language processing technology!\" )\n",
    "for token in doc:\n",
    "    print('{0}({1}) <-- {2} -- {3}({4})'.format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "    print(token.text, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[[1],[8]],[[1],[9]],[[1],[10]]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.],\n",
       "         [ 8.]],\n",
       "\n",
       "        [[ 1.],\n",
       "         [ 9.]],\n",
       "\n",
       "        [[ 1.],\n",
       "         [10.]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  2.,  2.,  3.,  4.,  8.],\n",
       "        [ 1.,  2.,  3.,  2.,  2.,  3.,  4.,  9.],\n",
       "        [ 1.,  2.,  3.,  2.,  2.,  3.,  4., 10.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(a, [a.shape[0], a.shape[1]*a.shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 2.],\n",
       "        [2., 3., 4., 8.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.1105e-04],\n",
       "         [9.9909e-01]],\n",
       "\n",
       "        [[3.3535e-04],\n",
       "         [9.9966e-01]],\n",
       "\n",
       "        [[1.2339e-04],\n",
       "         [9.9988e-01]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 3., 4., 5.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(torch.Tensor([1,4,3,4,5]), nn.Parameter(torch.ones(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "dataset = \"ohsumed\"\n",
    "save_path = 'temp_v1'\n",
    "corpus_path = 'corpus_v1'\n",
    "\n",
    "# param\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# with open(f\"stop_words.txt\", \"r\", encoding=\"latin1\") as f:\n",
    "#     stop_words = set(f.read().strip().split(\"\\n\"))\n",
    "\n",
    "# 词频\n",
    "least_freq = 5\n",
    "if dataset == \"mr\" or \"SST\" in dataset:\n",
    "    stop_words = set()\n",
    "    least_freq = 0\n",
    "\n",
    "\n",
    "# func load texts & labels\n",
    "def load_dataset(dataset):\n",
    "    with open(f\"{corpus_path}/{dataset}.texts.txt\", \"rb\") as f:\n",
    "        texts = f.read()\n",
    "        texts = texts.decode('utf-8', 'ignore')\n",
    "        texts = texts.strip().split(\"\\n\")\n",
    "    with open(f\"{corpus_path}/{dataset}.labels.txt\", \"r\") as f:\n",
    "        labels = f.read().strip().split(\"\\n\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def filter_text_old(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", text)\n",
    "    text = text.replace(\"'ll \", \" will \")\n",
    "    text = text.replace(\"'d \", \" would \")\n",
    "    text = text.replace(\"'m \", \" am \")\n",
    "    text = text.replace(\"'s \", \" is \")\n",
    "    text = text.replace(\"'re \", \" are \")\n",
    "    text = text.replace(\"'ve \", \" have \")\n",
    "    text = text.replace(\" can't \", \" can not \")\n",
    "    text = text.replace(\" ain't \", \" are not \")\n",
    "    text = text.replace(\"n't \", \" not \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"(\", \" ( \")\n",
    "    text = text.replace(\")\", \" ) \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "# def filter_text(string):\n",
    "#     \"\"\"\n",
    "#     Tokenization/string cleaning for all datasets except for SST.\n",
    "#     Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "#     \"\"\"\n",
    "#     string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "#     string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "#     string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "#     string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "#     string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "#     string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "#     string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "#     string = re.sub(r\",\", \" , \", string)\n",
    "#     string = re.sub(r\"!\", \" ! \", string)\n",
    "#     string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "#     string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "#     string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "#     string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "#     return string.strip().lower()\n",
    "\n",
    "\n",
    "def filter_text(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = string.replace(\" can't \", \" can not \")\n",
    "    string = string.replace(\" ain't \", \" are not \")\n",
    "    string = string.replace(\"n't \", \" not \")\n",
    "    string = string.replace(\",\", \" , \")\n",
    "    string = string.replace(\"!\", \" ! \")\n",
    "    string = string.replace(\"(\", \" ( \")\n",
    "    string = string.replace(\")\", \" ) \")\n",
    "    string = string.replace(\"?\", \" ? \")\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def pos_text(text: str):\n",
    "    pos = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(pos)\n",
    "\n",
    "\n",
    "def words_pos_list(texts, text_pos, word2index):\n",
    "    words_list = []\n",
    "    pos_list = []\n",
    "    for t, p in zip(texts, text_pos):\n",
    "        temp = []\n",
    "        temp_pos = []\n",
    "        t_split = t.split()\n",
    "        for i in range(0, len(t_split)):\n",
    "            if t_split[i] in word2index:\n",
    "                temp.append(t_split[i])\n",
    "                temp_pos.append(p[i][1])\n",
    "        words_list.append(temp)\n",
    "        pos_list.append(temp_pos)\n",
    "    return words_list, pos_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    texts, labels = load_dataset(dataset)\n",
    "\n",
    "    # handle texts\n",
    "    texts_clean = [filter_text(t) for t in texts]\n",
    "    text_pos = [pos_text(t) for t in texts_clean]\n",
    "\n",
    "    word2count = Counter([w for t in texts_clean for w in t.split()])\n",
    "    word_count = [[w, c] for w, c in word2count.items() if c >= least_freq and w not in stop_words]\n",
    "    word2index = {w: i for i, (w, c) in enumerate(word_count)}\n",
    "\n",
    "    # words_list = [[w for w in t.split() if w in word2index] for t in texts_clean]\n",
    "    words_list, pos_list = words_pos_list(texts_clean, text_pos, word2index)\n",
    "\n",
    "    pos2count = Counter([w for t in pos_list for w in t])\n",
    "    pos_count = [[w, c] for w, c in pos2count.items()]\n",
    "    pos2index = {w: i for i, (w, c) in enumerate(pos_count)}\n",
    "\n",
    "    texts_remove = [\" \".join(ws) for ws in words_list]\n",
    "\n",
    "    # labels 2 targets\n",
    "    label2index = {l: i for i, l in enumerate(set(labels))}\n",
    "    targets = [label2index[l] for l in labels]\n",
    "\n",
    "    # save\n",
    "    with open(f\"{save_path}/{dataset}.texts.clean.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(texts_clean))\n",
    "\n",
    "    with open(f\"{save_path}/{dataset}.texts.remove.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(texts_remove))\n",
    "\n",
    "    np.save(f\"{save_path}/{dataset}.targets.npy\", targets)\n",
    "    joblib.dump(word2index, f\"{save_path}/{dataset}.word2index.pkl\")\n",
    "    joblib.dump(pos2index, f\"{save_path}/{dataset}.pos2index.pkl\")\n",
    "    joblib.dump(pos_list, f\"{save_path}/{dataset}.texts.pos.pkl\")\n",
    "\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
