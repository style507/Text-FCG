{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 15:29:16 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-06-14 15:29:16 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2021-06-14 15:29:16 INFO: Use device: cpu\n",
      "2021-06-14 15:29:16 INFO: Loading: tokenize\n",
      "2021-06-14 15:29:16 INFO: Loading: pos\n",
      "2021-06-14 15:29:16 INFO: Loading: lemma\n",
      "2021-06-14 15:29:17 INFO: Loading: depparse\n",
      "2021-06-14 15:29:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, mwt, pos, lemma, depparse', tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "dataset = \"ohsumed\"\n",
    "save_path = 'nlp_temp_v2'\n",
    "corpus_path = 'corpus_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "window_size = 3\n",
    "embedding_dim = 300\n",
    "max_text_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param\n",
    "stop_words = set(stopwords.words('english') + s)\n",
    "least_freq = 5\n",
    "if dataset == \"mr\" or \"SST\" in dataset:\n",
    "    stop_words = set()\n",
    "    least_freq = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func load texts & labels\n",
    "def load_dataset(dataset):\n",
    "    with open(f\"{corpus_path}/{dataset}.texts.txt\", \"r\", encoding=\"latin1\") as f:\n",
    "        texts = f.read().strip().split(\"\\n\")\n",
    "    with open(f\"{corpus_path}/{dataset}.labels.txt\", \"r\") as f:\n",
    "        labels = f.read().strip().split(\"\\n\")\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_text(text: str):\n",
    "    pos = text.split()\n",
    "    return nltk.pos_tag(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_simple_version(string):\n",
    "#     string = re.sub(r\"\\\\\", \"\", string)\n",
    "#     string = re.sub(r\"\\'\", \"\", string)\n",
    "#     string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"dome petroleum's \\\\reaffirms dome mines stake for sale at right price spokesman 'says'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dome petroleum's \\\\reaffirms dome mines stake for sale at right price spokesman 'says'\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_str_simple_version(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_text(string):\n",
    "#     \"\"\"\n",
    "#     Tokenization/string cleaning for all datasets except for SST.\n",
    "#     Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "#     \"\"\"\n",
    "#     string = re.sub(r\"[^A-Za-z0-9().,!?\\'\\`]\", \" \", string)\n",
    "#     string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "#     string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "#     string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "#     string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "#     string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "#     string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "#     string = re.sub(r\",\", \" , \", string)\n",
    "#     string = re.sub(r\"!\", \" ! \", string)\n",
    "#     string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "#     string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "#     string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "#     string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "#     return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9().,!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = string.replace(\" can't \", \" can not \")\n",
    "    string = string.replace(\" ain't \", \" are not \")\n",
    "    string = string.replace(\"n't \", \" not \")\n",
    "    string = string.replace(\",\", \" , \")\n",
    "    string = string.replace(\"!\", \" ! \")\n",
    "    string = string.replace(\"(\", \" ( \")\n",
    "    string = string.replace(\")\", \" ) \")\n",
    "    string = string.replace(\"?\", \" ? \")\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle texts\n",
    "texts_clean = [clean_str_simple_version(filter_text(t)) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surfactant treatment of full term newborns with respiratory failure. surfactant inactivation has been shown to be a significant factor in animal models of lung injury and may also be important in some forms of respiratory failure in full term newborns. fourteen full term newborns with respiratory failure associated with pneumonia ( 7 patients ) and meconium aspiration syndrome ( 7 patients ) were treated with 90 mg kg of a calf lung surfactant extract , given intratracheally up to every 6 hours for a maximum of four doses. the group mean fraction of inspired oxygen ( fi02 ) before treatment was 0.99 0.01 sem , and the mean airway pressure ( map ) was 14.6 1.0 cm h2o. patients showed significant improvement in oxygenation after initial surfactant treatment , with the arterial alveolar oxygenation ratio ( a a ratio ) rising from 0.09 0.01 before surfactant treatment to 0.22 0.05 by 15 minutes ( p .03 ) and remaining improved for 6 hours. the oxygenation index , incorporating map as well as oxygen variables , also improved significantly from 26.2 3.1 to 11.2 1.7 at 15 minutes ( p less than .001 ) , with improvement sustained for more than 6 hours. chest radiographs were blindly scored from 0 ( normal ) to 5 ( severe opacification ) , and these improved with marginal significance after initial surfactant treatment ( from 2.9 0.2 to 2.5 0.2 , p .05 ) .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_clean[1150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Surfactant treatment of full-term newborns with respiratory failure.  Surfactant inactivation has been shown to be a significant factor in animal models of lung injury and may also be important in some forms of respiratory failure in full-term newborns.  Fourteen full-term newborns with respiratory failure associated with pneumonia (7 patients) and meconium aspiration syndrome (7 patients) were treated with 90 mg/kg of a calf lung surfactant extract, given intratracheally up to every 6 hours for a maximum of four doses.  The group mean fraction of inspired oxygen (FI02) before treatment was 0.99 +/- 0.01 SEM, and the mean airway pressure (MAP) was 14.6 +/- 1.0 cm H2O.  Patients showed significant improvement in oxygenation after initial surfactant treatment, with the arterial-alveolar oxygenation ratio (a/A ratio) rising from 0.09 +/- 0.01 before surfactant treatment to 0.22 +/- 0.05 by 15 minutes (P = .03) and remaining improved for 6 hours.  The oxygenation index, incorporating MAP as well as oxygen variables, also improved significantly from 26.2 +/- 3.1 to 11.2 +/- 1.7 at 15 minutes (P less than .001), with improvement sustained for more than 6 hours.  Chest radiographs were blindly scored from 0 (normal) to 5 (severe opacification), and these improved with marginal significance after initial surfactant treatment (from 2.9 +/- 0.2 to 2.5 +/- 0.2, P = .05). '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_pos_edge(texts_clean):\n",
    "    anti_rel = ['punct','cop','case','det','root','cc','mark','aux','advmod','fixed','nummod','aux:pass']\n",
    "    texts_spilt = []\n",
    "    texts_pos = []\n",
    "    texts_edge = [] \n",
    "    for t in tqdm(texts_clean):\n",
    "        doc = nlp(t)\n",
    "        temp_s = []\n",
    "        temp_p = []\n",
    "        temp_e = []\n",
    "        for word in doc.sentences[0].words:\n",
    "            temp_s.append(word.text)\n",
    "            temp_p.append(word.upos)\n",
    "            if word.deprel not in anti_rel:\n",
    "                temp_e.append((word.head, word.id))\n",
    "        texts_spilt.append(temp_s)\n",
    "        texts_pos.append(temp_p)\n",
    "        texts_edge.append(temp_e)\n",
    "    return texts_spilt, texts_pos, texts_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split, pos, edge = text_split_pos_edge(texts_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(split, f\"{save_path}/{dataset}.split.pkl\")\n",
    "joblib.dump(pos, f\"{save_path}/{dataset}.pos.pkl\")\n",
    "joblib.dump(edge, f\"{save_path}/{dataset}.edge.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = joblib.load(f\"{save_path}/{dataset}.split.pkl\")\n",
    "edge = joblib.load(f\"{save_path}/{dataset}.edge.pkl\")\n",
    "pos= joblib.load(f\"{save_path}/{dataset}.pos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = Counter([w for t in split for w in t])\n",
    "word_count = [[w, c] for w, c in word2count.items() if c >= least_freq and w not in stop_words]\n",
    "word2index = {w: i for i, (w, c) in enumerate(word_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_split_pos_edge(split, pos, edge, word2index):\n",
    "    split_list = []\n",
    "    pos_list = []\n",
    "    edge_list = []\n",
    "    for t, p, ed in tqdm(zip(split, pos, edge)):\n",
    "        temp = []\n",
    "        temp_pos = []\n",
    "        k = -1\n",
    "        for i in range(0, len(t)):\n",
    "            if t[i] in word2index:\n",
    "                temp.append(t[i])\n",
    "                temp_pos.append(p[i])\n",
    "            else:\n",
    "                temp_edge = []\n",
    "                k += 1\n",
    "                for e in ed:\n",
    "                    e = list(e)\n",
    "                    if e[0] == i+1-k or e[1] == i+1-k:\n",
    "                        continue\n",
    "                    if e[0] > i+1-k:\n",
    "                        e[0] = e[0] - 1\n",
    "                    if e[1] > i+1-k:\n",
    "                        e[1] = e[1] - 1\n",
    "                    temp_edge.append(tuple(e))\n",
    "                ed = temp_edge\n",
    "        split_list.append(temp)\n",
    "        pos_list.append(temp_pos)\n",
    "        edge_list.append(ed)\n",
    "    return split_list, pos_list, edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "def normalize_adj(adj):\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    # 忽略除零\n",
    "    with np.errstate(divide='ignore'):\n",
    "        d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    adj_normalized = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "    return adj_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, pad_len):\n",
    "    if len(seq) > pad_len:\n",
    "        return seq[:pad_len]\n",
    "    return seq + [0] * (pad_len - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [01:00, 121.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# 7400 bit\n",
    "split_list, pos_list, edge_list = words_split_pos_edge(split, pos, edge, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2count = Counter([w for t in pos_list for w in t])\n",
    "pos_count = [[w, c] for w, c in pos2count.items()]\n",
    "pos2index = {w: i for i, (w, c) in enumerate(pos_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_remove = [\" \".join(ws) for ws in split_list]\n",
    "\n",
    "# labels 2 targets\n",
    "label2index = {l: i for i, l in enumerate(set(labels))}\n",
    "targets = [label2index[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp_temp_v2/ohsumed.texts.pos.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "with open(f\"{save_path}/{dataset}.texts.clean.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts_clean))\n",
    "\n",
    "with open(f\"{save_path}/{dataset}.texts.remove.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts_remove))\n",
    "\n",
    "np.save(f\"{save_path}/{dataset}.targets.npy\", targets)\n",
    "joblib.dump(word2index, f\"{save_path}/{dataset}.word2index.pkl\")\n",
    "joblib.dump(pos2index, f\"{save_path}/{dataset}.pos2index.pkl\")\n",
    "joblib.dump(pos_list, f\"{save_path}/{dataset}.texts.pos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词典和对应序号\n",
    "word2index = joblib.load(f\"{save_path}/{dataset}.word2index.pkl\")\n",
    "# 词典和对应序号\n",
    "pos2index = joblib.load(f\"{save_path}/{dataset}.pos2index.pkl\")\n",
    "# # 数据集\n",
    "# with open(f\"nlp_temp/{dataset}.texts.remove.txt\", \"r\") as f:\n",
    "#     texts = f.read().strip().split(\"\\n\")\n",
    "pos = joblib.load(f\"{save_path}/{dataset}.texts.pos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "all_vectors = np.load(f\"source/glove.6B.{embedding_dim}d.npy\")\n",
    "all_words = joblib.load(f\"source/glove.6B.words.pkl\")\n",
    "all_word2index = {w: i for i, w in enumerate(all_words)}\n",
    "all_word2index_keys = list(all_word2index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_remain2index = []\n",
    "for aw in all_word2index_keys:\n",
    "    if word2index.get(aw) is not None:\n",
    "        same_remain2index.append(word2index[aw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_del = [',', '!', '(', ')', '?','\\'']\n",
    "same_del2index = []\n",
    "for sd in same_del:\n",
    "    if word2index.get(sd) is not None:\n",
    "        same_del2index.append(word2index[sd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same(target, words):\n",
    "    b = []\n",
    "    for index, nums in enumerate(words):\n",
    "        if nums == target:\n",
    "            b.append(index)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_same(words, same_del2index, same_remain2index):            \n",
    "    temp_edges = []\n",
    "    list_set_words = list(set(words))\n",
    "    for l in list_set_words:\n",
    "        if l not in same_del2index and l in same_remain2index:\n",
    "            temp = check_same(l, words)\n",
    "            if len(temp) > 1:\n",
    "                temp_edges += list(itertools.permutations(temp, 2))\n",
    "    return temp_edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建图\n",
    "inputs = []\n",
    "graphs = []\n",
    "inputs_pos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7400/7400 [00:16<00:00, 457.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(split_list))):\n",
    "    parse = edge_list[i]\n",
    "    words = [word2index[w] for w in split_list[i]]\n",
    "    words = words[:max_text_len]\n",
    "    \n",
    "    poses = [pos2index[w] for w in pos_list[i]]\n",
    "    poses = poses[:max_text_len]\n",
    "    \n",
    "    nodes = words\n",
    "    edges = []\n",
    "    for i in range(len(words)):\n",
    "        center = i\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if i != j and 0 <= j < len(words):\n",
    "                neighbor = j\n",
    "                edges.append((center, neighbor))\n",
    "                \n",
    "#     cs = connect_same(words, same_del2index, same_remain2index)\n",
    "#     edges += cs\n",
    "    \n",
    "    for p in parse:\n",
    "        p = list(p)\n",
    "        p[0] = p[0] - 1\n",
    "        p[1] = p[1] - 1\n",
    "        if p[0] != -1 and p[1] != -1 and p[0] < max_text_len and p[1] < max_text_len:\n",
    "            edges.append((p[1], p[0]))\n",
    "    #去重\n",
    "    edges = list(set(edges))\n",
    "    \n",
    "    edge_count = Counter(edges).items()\n",
    "     # 邻接矩阵\n",
    "    row = [x for (x, y), c in edge_count]\n",
    "    col = [y for (x, y), c in edge_count]\n",
    "    weight = [c for (x, y), c in edge_count]\n",
    "    # 归一化\n",
    "    adj = sp.csr_matrix((weight, (row, col)), shape=(len(nodes), len(nodes)))\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    weight_normalized = [adj_normalized[x][y] for (x, y), c in edge_count]\n",
    "    # 保存节点和图\n",
    "    inputs.append(nodes)\n",
    "    graphs.append([row, col, weight_normalized])\n",
    "    inputs_pos.append(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_inputs = [len(e) for e in inputs]\n",
    "len_graphs = [len(x) for x, y, c in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7400/7400 [00:00<00:00, 190241.19it/s]\n",
      "100%|██████████| 7400/7400 [00:00<00:00, 9344.82it/s] \n",
      "100%|██████████| 7400/7400 [00:00<00:00, 168639.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# padding input\n",
    "pad_len_inputs = max(len_inputs)\n",
    "pad_len_graphs = max(len_graphs)\n",
    "inputs_pad = [pad_seq(e, pad_len_inputs) for e in tqdm(inputs)]\n",
    "graphs_pad = [[pad_seq(ee, pad_len_graphs) for ee in e] for e in tqdm(graphs)]\n",
    "poses_pad = [pad_seq(e, pad_len_inputs) for e in tqdm(inputs_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_pad = np.array(inputs_pad)\n",
    "weights_pad = np.array([c for x, y, c in graphs_pad])\n",
    "graphs_pad = np.array([[x, y] for x, y, c in graphs_pad])\n",
    "poses_pad = np.array(poses_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "word_set = [index2word[i] for i in range(len(index2word))]\n",
    "oov = np.random.normal(-0.01, 0.01, embedding_dim)\n",
    "word2vec = [all_vectors[all_word2index[w]] if w in all_word2index else oov for w in word_set]\n",
    "# word2vec = [all_vectors[all_word2index[w]] if w in all_word2index else np.random.normal(-0.01, 0.01, embedding_dim) for w in word_set]\n",
    "word2vec.append(np.zeros(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos2vec\n",
    "index2pos = {i: w for w, i in pos2index.items()}\n",
    "pos_set = [index2pos[i] for i in range(len(index2pos))]\n",
    "pos2vec = [np.random.normal(-0.1, 0.1, embedding_dim) for w in pos_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "joblib.dump(len_inputs, f\"{save_path}/{dataset}.len.inputs.pkl\")\n",
    "joblib.dump(len_graphs, f\"{save_path}/{dataset}.len.graphs.pkl\")\n",
    "np.save(f\"{save_path}/{dataset}.inputs.npy\", inputs_pad)\n",
    "np.save(f\"{save_path}/{dataset}.graphs.npy\", graphs_pad)\n",
    "np.save(f\"{save_path}/{dataset}.weights.npy\", weights_pad)\n",
    "np.save(f\"{save_path}/{dataset}.word2vec.npy\", word2vec)\n",
    "np.save(f\"{save_path}/{dataset}.poses_pad.npy\", poses_pad)\n",
    "np.save(f\"{save_path}/{dataset}.pos2vec.npy\", pos2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: superficial\thead id: 2\thead: cultures\tdeprel: amod\n",
      "id: 2\tword: cultures\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: in\thead id: 6\thead: evaluations\tdeprel: case\n",
      "id: 4\tword: neonatal\thead id: 6\thead: evaluations\tdeprel: amod\n",
      "id: 5\tword: sepsis\thead id: 6\thead: evaluations\tdeprel: compound\n",
      "id: 6\tword: evaluations\thead id: 2\thead: cultures\tdeprel: nmod\n",
      "id: 7\tword: .\thead id: 2\thead: cultures\tdeprel: punct\n",
      "id: 8\tword: impact\thead id: 2\thead: cultures\tdeprel: appos\n",
      "id: 9\tword: on\thead id: 11\thead: decision\tdeprel: case\n",
      "id: 10\tword: antibiotic\thead id: 11\thead: decision\tdeprel: amod\n",
      "id: 11\tword: decision\thead id: 8\thead: impact\tdeprel: nmod\n",
      "id: 12\tword: making\thead id: 8\thead: impact\tdeprel: acl\n",
      "id: 13\tword: .\thead id: 16\thead: performed\tdeprel: punct\n",
      "id: 14\tword: the\thead id: 15\thead: authors\tdeprel: det\n",
      "id: 15\tword: authors\thead id: 16\thead: performed\tdeprel: nsubj\n",
      "id: 16\tword: performed\thead id: 2\thead: cultures\tdeprel: parataxis\n",
      "id: 17\tword: a\thead id: 19\thead: analysis\tdeprel: det\n",
      "id: 18\tword: retrospective\thead id: 19\thead: analysis\tdeprel: amod\n",
      "id: 19\tword: analysis\thead id: 16\thead: performed\tdeprel: obj\n",
      "id: 20\tword: of\thead id: 23\thead: cultures\tdeprel: case\n",
      "id: 21\tword: neonatal\thead id: 23\thead: cultures\tdeprel: amod\n",
      "id: 22\tword: superficial\thead id: 23\thead: cultures\tdeprel: amod\n",
      "id: 23\tword: cultures\thead id: 19\thead: analysis\tdeprel: nmod\n",
      "id: 24\tword: and\thead id: 26\thead: effect\tdeprel: cc\n",
      "id: 25\tword: their\thead id: 26\thead: effect\tdeprel: nmod:poss\n",
      "id: 26\tword: effect\thead id: 19\thead: analysis\tdeprel: conj\n",
      "id: 27\tword: on\thead id: 29\thead: decision\tdeprel: case\n",
      "id: 28\tword: antimicrobial\thead id: 29\thead: decision\tdeprel: amod\n",
      "id: 29\tword: decision\thead id: 26\thead: effect\tdeprel: nmod\n",
      "id: 30\tword: making\thead id: 26\thead: effect\tdeprel: acl\n",
      "id: 31\tword: during\thead id: 35\thead: period\tdeprel: case\n",
      "id: 32\tword: a\thead id: 35\thead: period\tdeprel: det\n",
      "id: 33\tword: nine\thead id: 34\thead: month\tdeprel: nummod\n",
      "id: 34\tword: month\thead id: 35\thead: period\tdeprel: compound\n",
      "id: 35\tword: period\thead id: 30\thead: making\tdeprel: obl\n",
      "id: 36\tword: at\thead id: 39\thead: hospital\tdeprel: case\n",
      "id: 37\tword: nashville\thead id: 39\thead: hospital\tdeprel: compound\n",
      "id: 38\tword: general\thead id: 39\thead: hospital\tdeprel: amod\n",
      "id: 39\tword: hospital\thead id: 35\thead: period\tdeprel: nmod\n",
      "id: 40\tword: .\thead id: 2\thead: cultures\tdeprel: punct\n",
      "id: 41\tword: they\thead id: 42\thead: obtained\tdeprel: nsubj\n",
      "id: 42\tword: obtained\thead id: 16\thead: performed\tdeprel: conj\n",
      "id: 43\tword: and\thead id: 44\thead: reviewed\tdeprel: cc\n",
      "id: 44\tword: reviewed\thead id: 42\thead: obtained\tdeprel: conj\n",
      "id: 45\tword: charts\thead id: 42\thead: obtained\tdeprel: obj\n",
      "id: 46\tword: of\thead id: 47\thead: infants\tdeprel: case\n",
      "id: 47\tword: infants\thead id: 45\thead: charts\tdeprel: nmod\n",
      "id: 48\tword: (\thead id: 49\thead: n\tdeprel: punct\n",
      "id: 49\tword: n\thead id: 47\thead: infants\tdeprel: appos\n",
      "id: 50\tword: 66\thead id: 49\thead: n\tdeprel: nummod\n",
      "id: 51\tword: )\thead id: 49\thead: n\tdeprel: punct\n",
      "id: 52\tword: having\thead id: 53\thead: paired\tdeprel: aux\n",
      "id: 53\tword: paired\thead id: 45\thead: charts\tdeprel: amod\n",
      "id: 54\tword: superficial\thead id: 53\thead: paired\tdeprel: obj\n",
      "id: 55\tword: (\thead id: 56\thead: skin\tdeprel: punct\n",
      "id: 56\tword: skin\thead id: 54\thead: superficial\tdeprel: appos\n",
      "id: 57\tword: and\thead id: 59\thead: gastric\tdeprel: cc\n",
      "id: 58\tword: or\thead id: 59\thead: gastric\tdeprel: cc\n",
      "id: 59\tword: gastric\thead id: 60\thead: aspirate\tdeprel: amod\n",
      "id: 60\tword: aspirate\thead id: 56\thead: skin\tdeprel: conj\n",
      "id: 61\tword: )\thead id: 60\thead: aspirate\tdeprel: punct\n",
      "id: 62\tword: and\thead id: 70\thead: cultures\tdeprel: cc\n",
      "id: 63\tword: deep\thead id: 70\thead: cultures\tdeprel: amod\n",
      "id: 64\tword: (\thead id: 65\thead: blood\tdeprel: punct\n",
      "id: 65\tword: blood\thead id: 70\thead: cultures\tdeprel: compound\n",
      "id: 66\tword: and\thead id: 68\thead: fluid\tdeprel: cc\n",
      "id: 67\tword: cerebrospinal\thead id: 68\thead: fluid\tdeprel: amod\n",
      "id: 68\tword: fluid\thead id: 65\thead: blood\tdeprel: conj\n",
      "id: 69\tword: )\thead id: 68\thead: fluid\tdeprel: punct\n",
      "id: 70\tword: cultures\thead id: 82\thead: positive\tdeprel: nsubj\n",
      "id: 71\tword: for\thead id: 73\thead: evaluation\tdeprel: case\n",
      "id: 72\tword: the\thead id: 73\thead: evaluation\tdeprel: det\n",
      "id: 73\tword: evaluation\thead id: 70\thead: cultures\tdeprel: nmod\n",
      "id: 74\tword: of\thead id: 77\thead: sepsis\tdeprel: case\n",
      "id: 75\tword: early\thead id: 77\thead: sepsis\tdeprel: amod\n",
      "id: 76\tword: onset\thead id: 77\thead: sepsis\tdeprel: amod\n",
      "id: 77\tword: sepsis\thead id: 73\thead: evaluation\tdeprel: nmod\n",
      "id: 78\tword: .\thead id: 82\thead: positive\tdeprel: punct\n",
      "id: 79\tword: superficial\thead id: 80\thead: cultures\tdeprel: amod\n",
      "id: 80\tword: cultures\thead id: 82\thead: positive\tdeprel: nsubj\n",
      "id: 81\tword: were\thead id: 82\thead: positive\tdeprel: cop\n",
      "id: 82\tword: positive\thead id: 16\thead: performed\tdeprel: conj\n",
      "id: 83\tword: for\thead id: 84\thead: pathogens\tdeprel: case\n",
      "id: 84\tword: pathogens\thead id: 82\thead: positive\tdeprel: obl\n",
      "id: 85\tword: (\thead id: 87\thead: streptococcus\tdeprel: punct\n",
      "id: 86\tword: any\thead id: 87\thead: streptococcus\tdeprel: det\n",
      "id: 87\tword: streptococcus\thead id: 84\thead: pathogens\tdeprel: dep\n",
      "id: 88\tword: or\thead id: 91\thead: negative\tdeprel: cc\n",
      "id: 89\tword: enteric\thead id: 90\thead: gram\tdeprel: amod\n",
      "id: 90\tword: gram\thead id: 91\thead: negative\tdeprel: compound\n",
      "id: 91\tword: negative\thead id: 87\thead: streptococcus\tdeprel: conj\n",
      "id: 92\tword: )\thead id: 87\thead: streptococcus\tdeprel: punct\n",
      "id: 93\tword: in\thead id: 94\thead: 15\tdeprel: case\n",
      "id: 94\tword: 15\thead id: 87\thead: streptococcus\tdeprel: nmod\n",
      "id: 95\tword: (\thead id: 97\thead: 66\tdeprel: punct\n",
      "id: 96\tword: 10\thead id: 97\thead: 66\tdeprel: compound\n",
      "id: 97\tword: 66\thead id: 94\thead: 15\tdeprel: nmod\n",
      "id: 98\tword: )\thead id: 97\thead: 66\tdeprel: punct\n",
      "id: 99\tword: of\thead id: 100\thead: cases\tdeprel: case\n",
      "id: 100\tword: cases\thead id: 94\thead: 15\tdeprel: nmod\n",
      "id: 101\tword: .\thead id: 106\thead: affected\tdeprel: punct\n",
      "id: 102\tword: antimicrobial\thead id: 103\thead: decision\tdeprel: amod\n",
      "id: 103\tword: decision\thead id: 104\thead: making\tdeprel: compound\n",
      "id: 104\tword: making\thead id: 106\thead: affected\tdeprel: nsubj:pass\n",
      "id: 105\tword: was\thead id: 106\thead: affected\tdeprel: aux:pass\n",
      "id: 106\tword: affected\thead id: 84\thead: pathogens\tdeprel: acl:relcl\n",
      "id: 107\tword: in\thead id: 109\thead: one\tdeprel: case\n",
      "id: 108\tword: only\thead id: 109\thead: one\tdeprel: advmod\n",
      "id: 109\tword: one\thead id: 106\thead: affected\tdeprel: obl\n",
      "id: 110\tword: of\thead id: 112\thead: cases\tdeprel: case\n",
      "id: 111\tword: these\thead id: 112\thead: cases\tdeprel: det\n",
      "id: 112\tword: cases\thead id: 109\thead: one\tdeprel: nmod\n",
      "id: 113\tword: ,\thead id: 119\thead: manner\tdeprel: punct\n",
      "id: 114\tword: and\thead id: 119\thead: manner\tdeprel: cc\n",
      "id: 115\tword: in\thead id: 119\thead: manner\tdeprel: case\n",
      "id: 116\tword: a\thead id: 119\thead: manner\tdeprel: det\n",
      "id: 117\tword: seemingly\thead id: 118\thead: inappropriate\tdeprel: advmod\n",
      "id: 118\tword: inappropriate\thead id: 119\thead: manner\tdeprel: amod\n",
      "id: 119\tword: manner\thead id: 109\thead: one\tdeprel: conj\n",
      "id: 120\tword: .\thead id: 125\thead: was\tdeprel: punct\n",
      "id: 121\tword: in\thead id: 122\thead: summary\tdeprel: case\n",
      "id: 122\tword: summary\thead id: 119\thead: manner\tdeprel: nmod\n",
      "id: 123\tword: ,\thead id: 125\thead: was\tdeprel: punct\n",
      "id: 124\tword: there\thead id: 125\thead: was\tdeprel: expl\n",
      "id: 125\tword: was\thead id: 106\thead: affected\tdeprel: conj\n",
      "id: 126\tword: no\thead id: 127\thead: evidence\tdeprel: det\n",
      "id: 127\tword: evidence\thead id: 125\thead: was\tdeprel: nsubj\n",
      "id: 128\tword: or\thead id: 129\thead: review\tdeprel: cc\n",
      "id: 129\tword: review\thead id: 127\thead: evidence\tdeprel: conj\n",
      "id: 130\tword: that\thead id: 137\thead: influenced\tdeprel: mark\n",
      "id: 131\tword: superficial\thead id: 132\thead: cultures\tdeprel: amod\n",
      "id: 132\tword: cultures\thead id: 137\thead: influenced\tdeprel: nsubj\n",
      "id: 133\tword: used\thead id: 132\thead: cultures\tdeprel: acl\n",
      "id: 134\tword: in\thead id: 136\thead: evaluation\tdeprel: case\n",
      "id: 135\tword: sepsis\thead id: 136\thead: evaluation\tdeprel: compound\n",
      "id: 136\tword: evaluation\thead id: 133\thead: used\tdeprel: obl\n",
      "id: 137\tword: influenced\thead id: 125\thead: was\tdeprel: parataxis\n",
      "id: 138\tword: physician\thead id: 140\thead: decision\tdeprel: compound\n",
      "id: 139\tword: antimicrobial\thead id: 140\thead: decision\tdeprel: amod\n",
      "id: 140\tword: decision\thead id: 137\thead: influenced\tdeprel: obj\n",
      "id: 141\tword: making\thead id: 137\thead: influenced\tdeprel: advcl\n",
      "id: 142\tword: in\thead id: 144\thead: case\tdeprel: case\n",
      "id: 143\tword: one\thead id: 144\thead: case\tdeprel: nummod\n",
      "id: 144\tword: case\thead id: 141\thead: making\tdeprel: obl\n",
      "id: 145\tword: they\thead id: 148\thead: led\tdeprel: nsubj\n",
      "id: 146\tword: may\thead id: 148\thead: led\tdeprel: aux\n",
      "id: 147\tword: have\thead id: 148\thead: led\tdeprel: aux\n",
      "id: 148\tword: led\thead id: 137\thead: influenced\tdeprel: conj\n",
      "id: 149\tword: to\thead id: 152\thead: exposure\tdeprel: case\n",
      "id: 150\tword: unnecessary\thead id: 152\thead: exposure\tdeprel: amod\n",
      "id: 151\tword: antibiotic\thead id: 152\thead: exposure\tdeprel: amod\n",
      "id: 152\tword: exposure\thead id: 148\thead: led\tdeprel: obl\n",
      "id: 153\tword: .\thead id: 2\thead: cultures\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word.id:1 head.id:2\n",
      "word.id:2 head.id:0\n",
      "word.id:3 head.id:6\n",
      "word.id:4 head.id:6\n",
      "word.id:5 head.id:6\n",
      "word.id:6 head.id:2\n",
      "word.id:7 head.id:2\n",
      "word.id:8 head.id:2\n",
      "word.id:9 head.id:11\n",
      "word.id:10 head.id:11\n",
      "word.id:11 head.id:8\n",
      "word.id:12 head.id:8\n",
      "word.id:13 head.id:16\n",
      "word.id:14 head.id:15\n",
      "word.id:15 head.id:16\n",
      "word.id:16 head.id:2\n",
      "word.id:17 head.id:19\n",
      "word.id:18 head.id:19\n",
      "word.id:19 head.id:16\n",
      "word.id:20 head.id:23\n",
      "word.id:21 head.id:23\n",
      "word.id:22 head.id:23\n",
      "word.id:23 head.id:19\n",
      "word.id:24 head.id:26\n",
      "word.id:25 head.id:26\n",
      "word.id:26 head.id:19\n",
      "word.id:27 head.id:29\n",
      "word.id:28 head.id:29\n",
      "word.id:29 head.id:26\n",
      "word.id:30 head.id:26\n",
      "word.id:31 head.id:35\n",
      "word.id:32 head.id:35\n",
      "word.id:33 head.id:34\n",
      "word.id:34 head.id:35\n",
      "word.id:35 head.id:30\n",
      "word.id:36 head.id:39\n",
      "word.id:37 head.id:39\n",
      "word.id:38 head.id:39\n",
      "word.id:39 head.id:35\n",
      "word.id:40 head.id:2\n",
      "word.id:41 head.id:42\n",
      "word.id:42 head.id:16\n",
      "word.id:43 head.id:44\n",
      "word.id:44 head.id:42\n",
      "word.id:45 head.id:42\n",
      "word.id:46 head.id:47\n",
      "word.id:47 head.id:45\n",
      "word.id:48 head.id:49\n",
      "word.id:49 head.id:47\n",
      "word.id:50 head.id:49\n",
      "word.id:51 head.id:49\n",
      "word.id:52 head.id:53\n",
      "word.id:53 head.id:45\n",
      "word.id:54 head.id:53\n",
      "word.id:55 head.id:56\n",
      "word.id:56 head.id:54\n",
      "word.id:57 head.id:59\n",
      "word.id:58 head.id:59\n",
      "word.id:59 head.id:60\n",
      "word.id:60 head.id:56\n",
      "word.id:61 head.id:60\n",
      "word.id:62 head.id:70\n",
      "word.id:63 head.id:70\n",
      "word.id:64 head.id:65\n",
      "word.id:65 head.id:70\n",
      "word.id:66 head.id:68\n",
      "word.id:67 head.id:68\n",
      "word.id:68 head.id:65\n",
      "word.id:69 head.id:68\n",
      "word.id:70 head.id:82\n",
      "word.id:71 head.id:73\n",
      "word.id:72 head.id:73\n",
      "word.id:73 head.id:70\n",
      "word.id:74 head.id:77\n",
      "word.id:75 head.id:77\n",
      "word.id:76 head.id:77\n",
      "word.id:77 head.id:73\n",
      "word.id:78 head.id:82\n",
      "word.id:79 head.id:80\n",
      "word.id:80 head.id:82\n",
      "word.id:81 head.id:82\n",
      "word.id:82 head.id:16\n",
      "word.id:83 head.id:84\n",
      "word.id:84 head.id:82\n",
      "word.id:85 head.id:87\n",
      "word.id:86 head.id:87\n",
      "word.id:87 head.id:84\n",
      "word.id:88 head.id:91\n",
      "word.id:89 head.id:90\n",
      "word.id:90 head.id:91\n",
      "word.id:91 head.id:87\n",
      "word.id:92 head.id:87\n",
      "word.id:93 head.id:94\n",
      "word.id:94 head.id:87\n",
      "word.id:95 head.id:97\n",
      "word.id:96 head.id:97\n",
      "word.id:97 head.id:94\n",
      "word.id:98 head.id:97\n",
      "word.id:99 head.id:100\n",
      "word.id:100 head.id:94\n",
      "word.id:101 head.id:106\n",
      "word.id:102 head.id:103\n",
      "word.id:103 head.id:104\n",
      "word.id:104 head.id:106\n",
      "word.id:105 head.id:106\n",
      "word.id:106 head.id:84\n",
      "word.id:107 head.id:109\n",
      "word.id:108 head.id:109\n",
      "word.id:109 head.id:106\n",
      "word.id:110 head.id:112\n",
      "word.id:111 head.id:112\n",
      "word.id:112 head.id:109\n",
      "word.id:113 head.id:119\n",
      "word.id:114 head.id:119\n",
      "word.id:115 head.id:119\n",
      "word.id:116 head.id:119\n",
      "word.id:117 head.id:118\n",
      "word.id:118 head.id:119\n",
      "word.id:119 head.id:109\n",
      "word.id:120 head.id:125\n",
      "word.id:121 head.id:122\n",
      "word.id:122 head.id:119\n",
      "word.id:123 head.id:125\n",
      "word.id:124 head.id:125\n",
      "word.id:125 head.id:106\n",
      "word.id:126 head.id:127\n",
      "word.id:127 head.id:125\n",
      "word.id:128 head.id:129\n",
      "word.id:129 head.id:127\n",
      "word.id:130 head.id:137\n",
      "word.id:131 head.id:132\n",
      "word.id:132 head.id:137\n",
      "word.id:133 head.id:132\n",
      "word.id:134 head.id:136\n",
      "word.id:135 head.id:136\n",
      "word.id:136 head.id:133\n",
      "word.id:137 head.id:125\n",
      "word.id:138 head.id:140\n",
      "word.id:139 head.id:140\n",
      "word.id:140 head.id:137\n",
      "word.id:141 head.id:137\n",
      "word.id:142 head.id:144\n",
      "word.id:143 head.id:144\n",
      "word.id:144 head.id:141\n",
      "word.id:145 head.id:148\n",
      "word.id:146 head.id:148\n",
      "word.id:147 head.id:148\n",
      "word.id:148 head.id:137\n",
      "word.id:149 head.id:152\n",
      "word.id:150 head.id:152\n",
      "word.id:151 head.id:152\n",
      "word.id:152 head.id:148\n",
      "word.id:153 head.id:2\n"
     ]
    }
   ],
   "source": [
    "for word in doc.sentences[0].words:\n",
    "    print(f'word.id:{word.id} head.id:{word.head}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'CCONJ',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'NUM',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'NUM',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'CCONJ',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADV',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'NOUN',\n",
       " 'SCONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'AUX',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.upos for word in doc.sentences[0].words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['superficial',\n",
       " 'cultures',\n",
       " 'in',\n",
       " 'neonatal',\n",
       " 'sepsis',\n",
       " 'evaluations',\n",
       " '.',\n",
       " 'impact',\n",
       " 'on',\n",
       " 'antibiotic',\n",
       " 'decision',\n",
       " 'making',\n",
       " '.',\n",
       " 'the',\n",
       " 'authors',\n",
       " 'performed',\n",
       " 'a',\n",
       " 'retrospective',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'neonatal',\n",
       " 'superficial',\n",
       " 'cultures',\n",
       " 'and',\n",
       " 'their',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'antimicrobial',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'during',\n",
       " 'a',\n",
       " 'nine',\n",
       " 'month',\n",
       " 'period',\n",
       " 'at',\n",
       " 'nashville',\n",
       " 'general',\n",
       " 'hospital',\n",
       " '.',\n",
       " 'they',\n",
       " 'obtained',\n",
       " 'and',\n",
       " 'reviewed',\n",
       " 'charts',\n",
       " 'of',\n",
       " 'infants',\n",
       " '(',\n",
       " 'n',\n",
       " '66',\n",
       " ')',\n",
       " 'having',\n",
       " 'paired',\n",
       " 'superficial',\n",
       " '(',\n",
       " 'skin',\n",
       " 'and',\n",
       " 'or',\n",
       " 'gastric',\n",
       " 'aspirate',\n",
       " ')',\n",
       " 'and',\n",
       " 'deep',\n",
       " '(',\n",
       " 'blood',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " ')',\n",
       " 'cultures',\n",
       " 'for',\n",
       " 'the',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'early',\n",
       " 'onset',\n",
       " 'sepsis',\n",
       " '.',\n",
       " 'superficial',\n",
       " 'cultures',\n",
       " 'were',\n",
       " 'positive',\n",
       " 'for',\n",
       " 'pathogens',\n",
       " '(',\n",
       " 'any',\n",
       " 'streptococcus',\n",
       " 'or',\n",
       " 'enteric',\n",
       " 'gram',\n",
       " 'negative',\n",
       " ')',\n",
       " 'in',\n",
       " '15',\n",
       " '(',\n",
       " '10',\n",
       " '66',\n",
       " ')',\n",
       " 'of',\n",
       " 'cases',\n",
       " '.',\n",
       " 'antimicrobial',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'was',\n",
       " 'affected',\n",
       " 'in',\n",
       " 'only',\n",
       " 'one',\n",
       " 'of',\n",
       " 'these',\n",
       " 'cases',\n",
       " ',',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'seemingly',\n",
       " 'inappropriate',\n",
       " 'manner',\n",
       " '.',\n",
       " 'in',\n",
       " 'summary',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'or',\n",
       " 'review',\n",
       " 'that',\n",
       " 'superficial',\n",
       " 'cultures',\n",
       " 'used',\n",
       " 'in',\n",
       " 'sepsis',\n",
       " 'evaluation',\n",
       " 'influenced',\n",
       " 'physician',\n",
       " 'antimicrobial',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'in',\n",
       " 'one',\n",
       " 'case',\n",
       " 'they',\n",
       " 'may',\n",
       " 'have',\n",
       " 'led',\n",
       " 'to',\n",
       " 'unnecessary',\n",
       " 'antibiotic',\n",
       " 'exposure',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.text for word in doc.sentences[0].words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
